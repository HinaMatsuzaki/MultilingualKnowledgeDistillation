## Title
Multilingual Knowledge Distillation for Efficient Extension of English Sentence Embedding Models to Japanese

## Abstract
In this research, I present an efficient methodology for extending pre-existing English sentence embedding models to the Japanese language. My approach leverages the concept of knowledge distillation, a technique of transferring knowledge from a "teacher" model to a "student" model. Specifically, I use the monolingual model to generate sentence embeddings for the source language (English), and then train the multilingual model using translated sentences (Japanese) to mimic the monolingual model. This approach is based on the idea that original sentences and their translated sentences should be mapped to the same locations in the vector space. This methodology offers the capability to extend pre-existing monolingual models to diverse languages even with limited training data.
